{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:17.505598Z","iopub.status.busy":"2021-10-20T19:59:17.505202Z","iopub.status.idle":"2021-10-20T19:59:21.092699Z","shell.execute_reply":"2021-10-20T19:59:21.092009Z","shell.execute_reply.started":"2021-10-20T19:59:17.505529Z"},"trusted":true},"outputs":[],"source":["# Import all required packages\n","\n","# helper packages\n","import numpy as np\n","import imageio\n","import matplotlib.pyplot as plt\n","import random\n","import cv2\n","import os\n","\n","# tensorflow packages\n","import tensorflow as tf \n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.models import load_model\n","\n","# import sklearn for data split convenience\n","from sklearn.model_selection import train_test_split\n","\n","# import own helper functions\n","from helper_functions import helpers\n","from tf_functions import tf_helpers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# List all available devices for the machi\n","\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:21.100694Z","iopub.status.busy":"2021-10-20T19:59:21.100502Z","iopub.status.idle":"2021-10-20T19:59:21.107438Z","shell.execute_reply":"2021-10-20T19:59:21.106705Z","shell.execute_reply.started":"2021-10-20T19:59:21.100671Z"},"trusted":true},"outputs":[],"source":["# create variables to store images as list\n","imagePaths = [\"./archive/\"+\"data\"+i+\"/\"+\"data\"+i+\"/CameraRGB/\" for i in ['A', 'B', 'C', 'D', 'E']]\n","maskPaths = [\"./archive/\"+\"data\"+i+\"/\"+\"data\"+i+\"/CameraSeg/\" for i in ['A', 'B', 'C', 'D', 'E']]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:21.120171Z","iopub.status.busy":"2021-10-20T19:59:21.119402Z","iopub.status.idle":"2021-10-20T19:59:21.148130Z","shell.execute_reply":"2021-10-20T19:59:21.147482Z","shell.execute_reply.started":"2021-10-20T19:59:21.120077Z"},"trusted":true},"outputs":[],"source":["# Create a list of file paths for the input images\n","imagePaths = helpers.createImagePaths(imagePaths)\n","\n","# Create a list of file paths for the corresponding mask images\n","maskPaths = helpers.createImagePaths(maskPaths)\n","\n","# Get the number of input images and corresponding mask images\n","nImg, nMask = len(imagePaths), len(maskPaths)\n","\n","# Print the number of input images and corresponding mask images\n","print(f\"Number of images and masks: {nImg}, {nMask}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:21.152088Z","iopub.status.busy":"2021-10-20T19:59:21.150589Z","iopub.status.idle":"2021-10-20T19:59:29.423516Z","shell.execute_reply":"2021-10-20T19:59:29.422834Z","shell.execute_reply.started":"2021-10-20T19:59:21.152050Z"},"trusted":true},"outputs":[],"source":["# Visualize some sample images alongside their mask\n","nSamples = len(imagePaths)\n","\n","for i in range(2):\n","    N = random.randint(0,nSamples-1)\n","\n","    img = imageio.imread(imagePaths[N])\n","    mask = imageio.imread(maskPaths[N])\n","    # Get the maximum value across the color channels for each pixel in the mask\n","    max_values = np.max(mask, axis=2)\n","\n","    # Reshape the resulting 2D array to have the same shape as the input image\n","    reshaped_mask = max_values.reshape(img.shape[0], img.shape[1])\n","\n","    # Assign the reshaped mask to the 'mask' variable\n","    mask = reshaped_mask\n","\n","    fig, arr = plt.subplots(1, 2, figsize=(18, 6))\n","    arr[0].imshow(img)\n","    arr[0].set_title('CARLA visualization')\n","    arr[0].axis(\"off\")\n","    arr[1].imshow(mask)\n","    arr[1].set_title('Image mask')\n","    arr[1].axis(\"off\")\n","    plt.savefig(\"img_\"+str(i)+\".png\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:29.425277Z","iopub.status.busy":"2021-10-20T19:59:29.424834Z","iopub.status.idle":"2021-10-20T19:59:29.438803Z","shell.execute_reply":"2021-10-20T19:59:29.438033Z","shell.execute_reply.started":"2021-10-20T19:59:29.425238Z"},"trusted":true},"outputs":[],"source":["# Split the dataset into three parts: training, validation and test\n","trainImagePaths, valImagePaths, trainMaskPaths, valMaskPaths = train_test_split(imagePaths, maskPaths, train_size=0.8, random_state=0)\n","valImagePaths, testImagePaths, valMaskPaths, testMaskPaths = train_test_split(valImagePaths, valMaskPaths, train_size=0.8, random_state=0)\n","\n","print(f'{len(trainImagePaths)} images in training')\n","print(f'{len(valImagePaths)} images in validation')\n","print(f'{len(testImagePaths)} images in testing')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:29.461252Z","iopub.status.busy":"2021-10-20T19:59:29.460666Z","iopub.status.idle":"2021-10-20T19:59:30.282201Z","shell.execute_reply":"2021-10-20T19:59:30.281381Z","shell.execute_reply.started":"2021-10-20T19:59:29.461214Z"},"trusted":true},"outputs":[],"source":["batchSize = 64\n","bufferSize = 500\n","nClasses = 13\n","# Use the dataGenerator function from the helpers module to create TensorFlow datasets for training, validation, and testing\n","trainDataset = helpers.dataGenerator(trainImagePaths, trainMaskPaths, bufferSize, batchSize)\n","valDataset = helpers.dataGenerator(valImagePaths, valMaskPaths, bufferSize, batchSize)\n","testDataset = helpers.dataGenerator(testImagePaths, testMaskPaths, bufferSize, batchSize)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:30.317792Z","iopub.status.busy":"2021-10-20T19:59:30.317561Z","iopub.status.idle":"2021-10-20T19:59:31.022704Z","shell.execute_reply":"2021-10-20T19:59:31.022015Z","shell.execute_reply.started":"2021-10-20T19:59:30.317769Z"},"trusted":true},"outputs":[],"source":["# Set the number of filters for the UNet model\n","filters = 32\n","\n","# Define the UNet model with the specified number of filters\n","model = tf_helpers.defineUNetModel((256, 256, 3), filters=filters, outputNodes=23)\n","\n","# Print a summary of the model architecture\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:31.024194Z","iopub.status.busy":"2021-10-20T19:59:31.023896Z","iopub.status.idle":"2021-10-20T19:59:31.041129Z","shell.execute_reply":"2021-10-20T19:59:31.040285Z","shell.execute_reply.started":"2021-10-20T19:59:31.024153Z"},"trusted":true},"outputs":[],"source":["# Compile the model with Adam optimizer, sparse categorical cross-entropy loss, and accuracy metric\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up an early stopping callback to stop training if validation accuracy does not improve for 5 epochs\n","callback = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","\n","# Set up a learning rate reduction callback to reduce the learning rate if validation accuracy does not improve for 3 epochs\n","reduceLr = ReduceLROnPlateau(monitor='val_accuracy',factor=0.1, patience=3, verbose=1, min_lr=0.00001)\n","\n","# Set the number of epochs to train for\n","epochs = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T19:59:31.043020Z","iopub.status.busy":"2021-10-20T19:59:31.042526Z","iopub.status.idle":"2021-10-20T20:27:21.093124Z","shell.execute_reply":"2021-10-20T20:27:21.092368Z","shell.execute_reply.started":"2021-10-20T19:59:31.042984Z"},"trusted":true},"outputs":[],"source":["history = model.fit(trainDataset, # training dataset\n","                    validation_data = valDataset, # validation dataset\n","                    epochs = epochs, # number of epochs to train for\n","                    verbose=1, # print progress bar for each epoch\n","                    callbacks = [callback, reduceLr], # list of callbacks to apply during training\n","                    batch_size = batchSize, # batch size\n","                    shuffle = True) # shuffle the training data at the beginning of each epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:27:21.095097Z","iopub.status.busy":"2021-10-20T20:27:21.094733Z","iopub.status.idle":"2021-10-20T20:27:21.425440Z","shell.execute_reply":"2021-10-20T20:27:21.424790Z","shell.execute_reply.started":"2021-10-20T20:27:21.095058Z"},"trusted":true},"outputs":[],"source":["# create a figure with size 10x10\n","plt.figure(figsize=(10, 10))\n","# create subplot 1 with 2 rows, 1 column, and index 1\n","plt.subplot(2, 1, 1)\n","# plot training accuracy\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","# plot validation accuracy\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","# set y-axis label\n","plt.ylabel('Accuracy')\n","# set y-axis limit to current minimum and 1\n","plt.ylim([min(plt.ylim()), 1])\n","# add legend to the plot in lower right position\n","plt.legend(loc='lower right')\n","# set plot title\n","plt.title('Training/Validation Accuracy')\n","\n","# create subplot 2 with 2 rows, 1 column, and index 2\n","plt.subplot(2, 1, 2)\n","# plot training loss\n","plt.plot(history.history['loss'], label='Training Loss')\n","# plot validation loss\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","# set y-axis label\n","plt.ylabel('Loss')\n","# set y-axis limit from 0 to 1.0\n","plt.ylim([0, 1.0])\n","# add legend to the plot in upper right position\n","plt.legend(loc='upper right')\n","# set plot title\n","plt.title('Training/Validation Loss')\n","# set x-axis label\n","plt.xlabel('Epochs')\n","# save the plot as image file\n","plt.savefig(\"loss_and_accuracy\")\n","# show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:27:21.427031Z","iopub.status.busy":"2021-10-20T20:27:21.426762Z","iopub.status.idle":"2021-10-20T20:27:21.776788Z","shell.execute_reply":"2021-10-20T20:27:21.775893Z","shell.execute_reply.started":"2021-10-20T20:27:21.426997Z"},"trusted":true},"outputs":[],"source":["# save the model so we don't have to retrain it every time \n","model.save('UNetOwnRecreation.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:27:21.778370Z","iopub.status.busy":"2021-10-20T20:27:21.778109Z","iopub.status.idle":"2021-10-20T20:27:49.349664Z","shell.execute_reply":"2021-10-20T20:27:49.348984Z","shell.execute_reply.started":"2021-10-20T20:27:21.778338Z"},"trusted":true},"outputs":[],"source":["# Evaluate model on the training dataset\n","trainLoss, trainAcc = model.evaluate(trainDataset, batch_size=batchSize)\n","\n","# Evaluate model on the validation dataset\n","valLoss, valAcc = model.evaluate(valDataset, batch_size=batchSize)\n","\n","# Evaluate model on the test dataset\n","testLoss, testAcc = model.evaluate(testDataset, batch_size=batchSize)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:27:49.351344Z","iopub.status.busy":"2021-10-20T20:27:49.351090Z","iopub.status.idle":"2021-10-20T20:27:49.357145Z","shell.execute_reply":"2021-10-20T20:27:49.356313Z","shell.execute_reply.started":"2021-10-20T20:27:49.351310Z"},"trusted":true},"outputs":[],"source":["print(f'Training acc: {trainAcc}')\n","print(f'Val acc: {valAcc}')\n","print(f'Test acc: {testAcc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:27:49.370790Z","iopub.status.busy":"2021-10-20T20:27:49.370490Z","iopub.status.idle":"2021-10-20T20:29:17.022586Z","shell.execute_reply":"2021-10-20T20:29:17.021664Z","shell.execute_reply.started":"2021-10-20T20:27:49.370764Z"},"trusted":true},"outputs":[],"source":["# Generate the ground truth and predicted masks for the training, validation, and test datasets\n","trainMasksTrue, trainMasksPred = tf_helpers.createMaskForImage(trainDataset, model)\n","valMasksTrue, valMasksPred = tf_helpers.createMaskForImage(valDataset, model)\n","testMasksTrue, testMasksPred = tf_helpers.createMaskForImage(testDataset, model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:29:17.065022Z","iopub.status.busy":"2021-10-20T20:29:17.064734Z","iopub.status.idle":"2021-10-20T20:29:40.562564Z","shell.execute_reply":"2021-10-20T20:29:40.561853Z","shell.execute_reply.started":"2021-10-20T20:29:17.064987Z"},"trusted":true},"outputs":[],"source":["# Initialize ModelEvaluation objects for the training, validation, and testing sets, and print their evaluation results\n","modelEvalTrain = tf_helpers.modelEvaluation(trainMasksTrue, trainMasksPred, n_classes=nClasses) # Instantiate a ModelEvaluation object for the training set\n","modelEvalVal = tf_helpers.modelEvaluation(valMasksTrue, valMasksPred, n_classes=nClasses) # Instantiate a ModelEvaluation object for the validation set\n","modelEvalTest = tf_helpers.modelEvaluation(testMasksTrue, testMasksPred, n_classes=nClasses) # Instantiate a ModelEvaluation object for the testing set\n","\n","# Print the evaluation results for each set\n","print(modelEvalTrain, modelEvalVal, modelEvalTest)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T20:29:46.575260Z","iopub.status.busy":"2021-10-20T20:29:46.575002Z","iopub.status.idle":"2021-10-20T20:29:47.384399Z","shell.execute_reply":"2021-10-20T20:29:47.383605Z","shell.execute_reply.started":"2021-10-20T20:29:46.575232Z"},"trusted":true},"outputs":[],"source":["# Load our model\n","model = load_model('UNetOwnRecreation.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Read in an image file and preprocess it\n","image = tf.io.read_file('<ENTER YOUR IMAGE FILE HERE>') # Read the image file\n","image = tf.image.decode_png(image, channels=3) # Decode the image\n","image = tf.image.convert_image_dtype(image, tf.float32) # Convert the image to floats between 0 and 1\n","image = tf.image.resize(image, (256, 256), method='nearest') # Resize the image to (256, 256) using nearest neighbor interpolation\n","\n","# Make a prediction on the preprocessed image\n","pred = model.predict(tf.expand_dims(image, 0))\n","\n","# Convert the prediction to a mask\n","predMask = tf.argmax(pred, axis=-1)\n","predMask = tf.expand_dims(predMask, axis=-1)\n","\n","# Extract the predicted mask from the batch and plot it alongside the input image and true mask\n","finalPred = predMask[0]\n","plt.figure(figsize=(18, 18))\n","title = ['Input Image', 'True Mask', 'Predicted Mask']\n","plt.title('Test')\n","plt.imshow(tf.keras.utils.array_to_img(finalPred))\n","plt.axis('off')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Forecast on our own video\n","sampleVideo = '<INSERT SAMPLE VIDEO HERE>' # .mp4 format\n","framesFolder = '<INSERT SAMPLE FRAMES FOLDER HERE>' # e.g. 'highway'\n","predFramesFolder = '<INSERT SAMPLE MASKS FOLDER HERE>' # e.g. 'highway_masks'\n","outputVideoFile = '<INSERT OUTPUT FILE NAME HERE>' # e.g. 'highway_forecast'\n","\n","# Open the sample video and read its frames\n","vidcap = cv2.VideoCapture(sampleVideo)\n","success, image = vidcap.read()\n","count = 0\n","\n","# Loop over each frame in the video and save it as an image\n","while success:\n","    cv2.imwrite(framesFolder + \"/frame%d.jpg\" % count, image)     \n","    success, image = vidcap.read()\n","    if count % 1000 == 0:\n","        print('Frame:', count)\n","    count += 1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Loop over each saved frame in the specified directory\n","for filename in range(count):\n","    f = os.path.join(framesFolder, \"frame\" + str(filename) + \".jpg\")\n","    \n","    # If the file exists, process it\n","    if os.path.isfile(f):\n","        image = tf.io.read_file(f)\n","        image = tf.image.decode_png(image, channels=3)\n","        image = tf.image.convert_image_dtype(image, tf.float32)\n","        image = tf.image.resize(image, (256, 256), method='nearest')\n","\n","        pred = model.predict(tf.expand_dims(image, 0))\n","        predMask = tf.argmax(pred, axis=-1)\n","        predMask = tf.expand_dims(predMask, axis=-1)\n","\n","        finalPred = predMask[0]\n","        plt.imshow(tf.keras.utils.array_to_img(finalPred))\n","        plt.savefig(predFramesFolder + '/frame' + str(filename) + \".jpg\")\n","        plt.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["frameArray = []\n","\n","# get all the image files in the predicted frames folder\n","files = [f for f in os.listdir('./' + predFramesFolder + '/')]\n","# sort the filenames by their numerical order\n","files.sort(key = lambda x: int(x[5:-4]))\n","\n","# loop through each file\n","for i in range(len(files)):\n","    filename='./' + predFramesFolder + '/' + files[i]\n","    # read the image file using OpenCV\n","    img = cv2.imread(filename)\n","    height, width, layers = img.shape\n","    size = (width,height)\n","    frameArray.append(img)\n","\n","# create a video writer object to write the predicted frames to a video file\n","out = cv2.VideoWriter(outputVideoFile + '.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n","\n","# loop through each predicted frame and write it to the output video file\n","for i in range(len(frameArray)):\n","    out.write(frameArray[i])\n","\n","# release the video writer object\n","out.release()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
